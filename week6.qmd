---
title: "Classification I "
format:
  html:
    toc: true    # Enable Table of Contents
    toc-depth: 2 # Show up to level 2 headings
---

## Summary

Todayâ€™s lecture is exploring remote sensing classification. Several dataset and applications mentioned are:

-   Urban expansion/sprawl with landsat data

-   Surface temperature (land and sea) with sentinel 3

-   Major air pollutants with sentinel 5

-   Urban green spaces (high medium res, Lidar)

-   Monitoring and illegal practices landsat data

-   Forest fires using landsat TM image

There are several steps to extract land cover (do classification) from earth observation data. We usually combined remote sensing with machine learning techniques that include training data, build classification model, output. Todayâ€™s class focus on classification and regression trees (CART) as a method of classification. It composes with 2 things :

a\. Classification trees (yes no/ discrete values)

In this method, we will have predictors and target. The decision is mapped with conditions that led to yes or no decision

b.Regression trees ( predict continuous dependant variable)

Meanwhile, in linear regression we try to fit into the regression line, however if we have data that isnâ€™t fitted into the data we subset this data into smaller chunks. When creating a decision tree, we would be mix all the leafes of several categories, quantified using gini impurity. And we aim to find the lowest gini impurity. We decide the breaks by creating imaginary vertical line among the data and identify the lowest sum of square residuals.

Things to be paid attention at = OVERFITTING, dealing with this could be

a\. Set the min number of pixels/ observations, Only split observations withÂ **min number of 20**.

b\. Pruning

c\. Removes leave, increase alfa, and find the lowest tree score. The idea is could we keep the accuracy, when we do generalizations

d\. Dividing data : training (creating trees) and testing data (applied the trees with testing data) find the lowest sum residuals.

See the picture :

We could pick some pixels as training data and validation,the machine learning will predict the value among this train and validation pixels.

Random forest

As decision tree is not good with new data, thus we could use another method is random forest. I like to use the chatgpt analogy on this:

ðŸ’¡ A Decision Tree is like asking one expert for advice.\
ðŸŒ² A Random Forest is like asking 100 experts, each with a unique take, then averaging their answers.

Applying into imagery:

a\. Supervised ( giving them a train data)

Support vector machine, the idea is like a logistic regression. Â It finds the **best boundary (decision boundary)** that separates different classes in the data.The benefit of this method is it allows some misclassification

b\. Unsupervised (I have train data, and I want 10 classification method) \> see recording (43â€™)

a\. Usually refers to clustering/ k-means.

b\. Isodata (same with kmeans but add several inputs such as ( cluster, iterations)

Repeat until all pixels were classified

## Application

## Reflection

## References
