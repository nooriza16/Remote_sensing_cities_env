% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Sneak Peak},
  pdfauthor={Nooriza Maharani},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Sneak Peak}
\author{Nooriza Maharani}
\date{January 21, 2025}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[enhanced, borderline west={3pt}{0pt}{shadecolor}, interior hidden, boxrule=0pt, breakable, frame hidden, sharp corners]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{rs-learning-diary}{%
\chapter{RS Learning Diary}\label{rs-learning-diary}}

This is a Quarto book to document my learning journey in \textbf{Remote
Sensing Cities and Environments} course during my time at CASA UCL
24/25, offering insights learned, its applications, and my own
reflections. The module is based on Dr Andrew Maclachlan github page
{[}\href{https://andrewmaclachlan.github.io/CASA0023/.}{here}{]}.

*For those of you who also want to learn Geographic Information Science
beyond `typical GIS' Software, as in use R-Studio, you could also visit
his other github page
{[}\href{https://andrewmaclachlan.github.io/CASA0005repo/index.html}{here}{]}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Hi, I'm Nooriza, a student currently pursuing a Master's degree in Urban
Spatial Science at UCL. I graduated with a degree in Geography,
specializing in Regional Development Studies. Over the past five years,
I have worked as a technical consultant for various governments in
Indonesia, ensuring that their decision-making is data-driven. Although
the results often need to pass through political and budgeting reviews,
that's essentially what I do.

Honestly, I don't have a specific topic of interest to add here because
I have many! That's why I chose an interdisciplinary course at CASA. One
thing for sure is that I love doing analytics for social good. In the
end, all the sophisticated methodologies and cutting-edge technical
tools are meaningful when we use them to address challenges and solve
problems, right?

\hypertarget{why-do-i-choose-this-module}{%
\section{Why do I choose this
module?}\label{why-do-i-choose-this-module}}

The reason I choose remote sensing is because I want to use its vast
open resources to analysis various topics. I had learned the foundations
during my undergraduate degree but I haven't delved further into it and
haven't got any experience to use GEE yet. Thus, I hope at the end of
this class, I will get knowledge on to get alternative of spatial data
using remote sensing plus analyse various topic across different scale
using GEE.

Remote sensing is also an interesting field as it could produce wealth
of information without direct contact. Don't you think learning remote
sensing makes us have the eye of the bird even beyond? I mean we agree
that remote sensing offers perspectives far beyond what our human eyes
can naturally perceive : \emph{allowing} to \emph{see things from above
and to see the unseen of the naked eye.}

\includegraphics[width=3.76042in,height=\textheight]{images/clipboard-2217286199.png}

For example, see the ASTER images of San Fransisco Bay below it
highlights different object such as vegetation (upper left); soil \&
rocks in mountainous area (upper right); urban materials (lower left) ;
and water temperature (lower right). It's cool !

\begin{figure}

\href{https://photojournal.jpl.nasa.gov/jpegMod/PIA02605_modest.jpg}{\includegraphics[width=3.78125in,height=\textheight]{images/PIA02605_modest.jpg}}

\end{figure}

Figure 1 : ASTER images of San Fransisco. source :
\href{https://photojournal.jpl.nasa.gov/catalog/PIA02605}{NASA/JPL}

Practically, learning this course will, hopefully, help me address the
challenges I faced during my previous work in Indonesia. For example,
while working on a project focused on healthcare accessibility across
hundreds of small islands, we struggled to obtain real-time data to
identify which islands were inhabited and which were not. Additionally,
we faced challenges in determining which islands had ports suitable for
docking ships. I believe that applying remote sensing data is both cost-
and time-efficient in helping the government maintain more precise and
up-to-date data, which is particularly important in world's largest
archipelago country like Indonesia.

Feel free to explore my site to learn more about my learning experience.
Hope it helps!

\bookmarksetup{startatroot}

\hypertarget{getting-to-know-remote-sensing}{%
\chapter{Getting to Know Remote
Sensing}\label{getting-to-know-remote-sensing}}

\hypertarget{summary}{%
\section{\texorpdfstring{\textbf{Summary}}{Summary}}\label{summary}}

This week, the lecture covers an introduction of remote sensing, such as
its vast application, instruments, collection method, and things we have
to consider when we deal with remote sensing data. I tried to make the
summary using visualization below to make it easier to understand.

\includegraphics{images/clipboard-1084697230.png}

During the practical, we explore several tools to deal with remote
sensing data such as SNAP (Sentinel Application Platform) and R-studio
to plot spectral signature. We are also introduced with 2 imagery :
Sentinel-2A and Landsat-8. It is interesting how this two imagery has a
global coverage and for FREE. Both of them has spectral bands that could
be useful for vegetation monitoring, land cover classification, and
agricultural applications. We could benefit from Sentinel-2 frequent
observations to monitor rapid changes. Meanwhile, Landsat data allows us
to do large areas and long-term vegetation monitoring as it has
extensive historical archive and consistent global coverage. Below I
discussed the application of both Landsat and Sentinel in a vegetation
analysis.

\hypertarget{application}{%
\section{\texorpdfstring{\textbf{Application}}{Application}}\label{application}}

\begin{quote}
\textbf{Landsat for monitoring accross vast region : Detecting of
vegetation evolution across China Urban Development}
\end{quote}

\begin{itemize}
\item
  When I mentioned Landsat have a vast amount of historical data,
  @han2025 explores this historical archive of 30 years Landsat data
  (spanning of landsat 5 to 8) on 2.125 city to monitor the vegetation
  evolution, using reflective bands such as Blue, green, red, NIR and
  SWIR (1 and 2) and highlighting vegetation characteristics using NDVI,
  EVI, and OSAVI. The NDVI and RGB bands were further processed to
  derive texture variables, including variance, contrast, entropy,
  angular second moment, and correlation. These texture metrics capture
  spatial patterns and fine-scale structural details of urban vegetation
  that may not be visible through spectral bands alone. The findings
  will classify vegetation in urban area, whether it is decreasing or
  increasing over time. I genuinely believe this finding has the
  potential to serve as a framework for evaluating the effectiveness of
  the government's long-term plan on urban greening. For instance in my
  country, Indonesia we have long-term regional planning that spanning
  for 20 years (reviewed every 5 years), the analyisis will help the
  policy maker to formulate more measured-target.

  \includegraphics[width=6.96875in,height=\textheight]{images/clipboard-1072383063.png}

  Figure 1: A sample result shows urban vegetation degradation in
  Hangzhou and an increase in vegetation in Zhengzhou. source : (Han et
  al. 2025).
\end{itemize}

However, Landsat is an optical imaging system that is often susceptible
to cloud cover and has limitations in distinguishing different
vegetation types based solely on spectral characteristics. Imagine
studying a mountainous region where cloud cover is persistent---using
optical images like Landsat for vegetation monitoring and identification
would be challenging.

To address this issue, Li et al.~(2023) utilized Sentinel-1, which
operates with C-band Synthetic Aperture Radar (SAR), enabling vegetation
mapping under all weather conditions. The SAR data from Sentinel-1, when
combined with the optical imagery of Sentinel-2, allows for the
production of high-resolution maps that effectively differentiate bamboo
forests from other vegetation types. This integration helps overcome the
limitations of optical data in vegetation monitoring, where mixed
spectral characteristics often lead to uncertainty in distinguishing
bamboo from other forest types.

\hypertarget{reflection}{%
\section{Reflection}\label{reflection}}

\begin{quote}
Remote sensing provides diversity in data source, but\ldots. will the
implementation be easy?
\end{quote}

After exploring the application of the two selected satellites during
practical this week, I have concluded that remote sensing data is
particularly effective for analyzing large-scale and long-term
variations. It can also help mitigate the high costs of manual data
collection across vast regions. This insight made me reflect on a
similar challenge in my country, Indonesia. We often have challenges to
find dataset for spatial analysis as we rely much on vector data, if any
it would be outdated. Using remote sensing data not only allows us to
have more updated data but also allows us to explore various potential
variables derived from satellite imagery : diversity in data sources. In
my country, the Geospatial Agency has already utilized remote sensing to
address time and cost constraint during data collection. It is legally
published as a baseline regulation to address the conventional method
for the collection and processing of geospatial data on shallow marine
habitat. They use remote sensing to create a work map for on-field
mapping habitat, enabling to be time and cost efficient.

However I must admit despite the potential of remote sensing, its
adoption in the government sector---especially at the local
level---remains limited. From my experience, this is largely due to a
lack of human resources with the skills to process and analyze remote
sensing data. For end users, the adoption of remote sensing is linked
with the information they can bring on the table, and often might depend
on the leadership, budgetary constraints, procedures and personal
capacity{[}@nationalacademyofsciences.2003{]}.

\hypertarget{references}{%
\section{References}\label{references}}

\bookmarksetup{startatroot}

\hypertarget{xaringan-and-quarto-book}{%
\chapter{Xaringan and Quarto Book}\label{xaringan-and-quarto-book}}

Lecture this week reminded me of one of powerful figure in Uchiha Clan,
the one who can manipulate reality once he activates this-so-called
Xaringan. Well, but this Xaringan is not related to figures in Konoha's
world but related to a certain library in R Studio that enable us to
create neat HTML slides in R.

I think a presentation is basically a way to communicate insights to the
audience, and a great presentation may even ``hypnotize'' the audience.
Is that one of reasons it called Xaringan?

\hypertarget{summary-1}{%
\section{Summary}\label{summary-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{xaringanExtra}\SpecialCharTok{::}\FunctionTok{embed\_xaringan}\NormalTok{(}\AttributeTok{url =} \StringTok{"https://nooriza16.github.io/Xaringan/Xaringan.html"}\NormalTok{, }\AttributeTok{ratio =} \StringTok{"16:9"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{reflections}{%
\section{Reflections}\label{reflections}}

For someone who is not familiar with html, learning Xaringan is
definitely challenging compared to powerpoint, as we just usually click
tabs on power point. Honestly, I still consider power point provides
more themes and more visualization effects that is easily to access
compared to Xaringan. However, as I delved further I realize that using
Xaringan is providing us with flexibility even such as positioned our
picture. So far, I feel like Xaringan is best at incorporating snippet
code on presentation or interactive features that usually too heavy to
load in power point. Besides, it helps me to give a sense of what html
look like.

\bookmarksetup{startatroot}

\hypertarget{image-correction}{%
\chapter{Image Correction}\label{image-correction}}

\hypertarget{summary-2}{%
\section{Summary}\label{summary-2}}

Here is my note based on this week's lecture that explores steps people
usually do in image correction. Although sometimes we get a
``ready-to-use data'' without the need to going through all of these
process, having the basic understandings of these steps would help us
understand the quality our data.

\includegraphics{images/clipboard-3039716080.png}

I also add several new terminologies, based on my own note during the
lecture, related to image processing during this week's summary

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.0935}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.9065}}@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Reflectance and radiance} & Reflectance is basically the amount
of light when a surface reflect the light, while radiance is the amount
of light captured by sensor after interacting with Earth's surface \\
\textbf{Digital Number} & A raw value for a given pixel that represents
the intensity of radiation received in a specific spectral band.

Digital number is important because it serves as a basis for image
classification, for example digital number close to 0 represents object
that absorbs much incoming light (low reflectance) such as water bodies
or shadows. \\
\textbf{Digital Object Substraction (DOS)} & DOS is an atmospheric
correction method that subtracts pixel values based on the amount of
difference between digital values of dark objects (usually water bodies)
with their corresponding reflectance \\
\textbf{Collection, level, and tier} & I will use the Landsat case to
explain this terminology. In Landsat the collection would be named as
Collection 1 and 2, it represents the sequence of launching time and
their mission : Landsat 2 is the latest. Level 1 is a scaled digital
number, while level 2 is further processed data. Meanwhile, tier 1 is
the highest quality data from Landsat and suitable for time series
analysis.

source:
\url{https://www.usgs.gov/landsat-missions/landsat-collection-2-level-1-data} \\
\end{longtable}

\hypertarget{application-1}{%
\section{Application}\label{application-1}}

In this week application, I would like to explore the application of
remote sensing on my favorite topic : data unavailability. In developing
countries like Indonesia, maintaining updated and comprehensive data is
challenging due to time and cost constraints. A study in Pakistan also
mentioned similar challenges, particularly data related to socioeconomic
condition that often limited and sporadic Arshad et al. (2023).
Honestly, this topic piques my interest because Bill Gates came across
an article in The Washington Post and seems fascinated with the idea of
using remote sensing to identify wealth, as he shared on X.

\includegraphics[width=2.66667in,height=\textheight]{images/clipboard-3275173560.png}

Figure 1 : Tweet about satellite imagery on detecting poor region.
Source : \url{https://x.com/yohaniddawela/status/1857398431146299437}

Arshad et al. (2023) addressed this issue using imagery that
automatically could derive insights about poverty. They use publicly
accessible high-resolution satellite imagery (Google Maps API with 16
zoom) and Landsat 7 (low resolution data). Google Maps API provides
high-resolution imagery to identify man-made features like buildings and
highway which are indicators of development levels meanwhile Landsat 7
is used to train Convolutional Neural Networks (a method of feature
extraction from imagery in Machine Learning) in identifying nightlight
bin (low to high). Areas with higher levels of economic activity and
development tend to have more lights at night. This method will produce
a map that indicates a poverty and development level compared to poverty
line. The results are then validated using actual socioeconomic data
from surveys. Below is a map where each point represents a poverty
clusters (10x10 km area), comparing predicted and actual data. Green
points indicate clusters above the poverty line, while red points
indicates the opposite.

\includegraphics[width=6.25in,height=\textheight]{images/clipboard-4160873008.png}

Figure 2 : Socioeconomic conditions compared to poverty line in
Pakistan. Source : (Arshad et al. 2023).

When reflecting on the map above, I would prefer to visualize the
classification results as polygon rather than points, as they were more
intuitive. Additionally, it would be nice to map the difference across
the years, Ben Abbes et al. (2024) just do this ! They use multispectral
images (Landsat 5, 7,8) and Nightlight images (from Defense
Meteorological Satellite Program (DMSP) and the Visible Infrared Imaging
Radiometer Suite (VIIRS)) in Southeast Brazil. The classification result
is represented using estimated wealth index and they could even map the
socioeconomic transformation across 10 years in a single map!

\includegraphics[width=6.03125in,height=\textheight]{images/clipboard-422155352.png}

Figure 3 : Spatio-temporal mapping of wealth index estimations in
Southeast Brazil. Source : Ben Abbes et al. (2024).

\hypertarget{reflection-1}{%
\section{Reflection}\label{reflection-1}}

I think performing Remote Sensing correction on R Studio is quite
challenging, as I become more used to using `button' in Remote Sensing
application such as ENVI or SNAP. After this week's lecture, I genuinely
think that Remote Sensing is quite complex as it is not only an image
but beyond the imagery each pixel is composed by digital number and it
could be linked and better interpreted using regression too.
Understanding the image classification using machine learning is also
quite challenging for me, as they use new terminologies that I haven't
heard before such as convolutional neural network, epoch, and data
training.

Honestly, remote sensing combined with machine learning quite scare me
off. However, I try to look beyond the methodology instead focusing on
how explanatory remote sensing can be when combined with classification
and prediction task, tasks machine learning good at. I think looking
beyond the methodology and focusing on the exciting application has also
helped me me thrive on managing challenge during this Master's. After
reading the paper, I tried to delve further into the combination of
remote sensing and machine learning. If I could turn back the time, I
would like to deploy this combination to make my works faster. I
recalled during my works years ago, the project needed to identify
thousand ports across hundred of islands in Indonesia, and we did that
manually ! If I understand correctly, I could combine image
classification techniques, such as convolutional neural networks, with
high-resolution imagery to detect local ports used for docking ship
(typically made of wood or cement) in the ocean.

\includegraphics{images/clipboard-2420843413.png}

Figure 3 : Google Earth Imagery portraying local ports in Anambas
Islands. Source : Google Earth, 2024

\bookmarksetup{startatroot}

\hypertarget{policy}{%
\chapter{Policy}\label{policy}}

\textbf{Project Case : A New Relocated Capital City of Indonesia ; From
Jakarta to Nusantara}

\includegraphics[width=9.82292in,height=\textheight]{images/clipboard-546673402.png}

Source :
\href{https://www.nytimes.com/interactive/2023/05/16/headway/indonesia-nusantara-jakarta.html}{www.nytimes.com}

\hypertarget{summary-3}{%
\section{Summary}\label{summary-3}}

Recently Indonesia planned to move its capital city from Jakarta (in
Java islands) into Penajam Paser Utara City (Borneo Islands), as the
current capital city, Jakarta, faced an issue of sinking, land
subsidence, overcrowding, low air and water quality (Bappenas 2021). The
term Nusantara is used to name this new capital city, symbolizing the
varied geographic settings and cultural diversities of Indonesia.

As for the time this published, Nusantara Development is on the phase 2
(2025-2029) that involved strengthening core area (housing, office,
commercial zone). Thus, in the time being, Jakarta will still remain the
capital of Indonesia until the Presidential Decree on the transfer of
the capital to Nusantara is issued. The issuance of this decree will
depend on the readiness of the new capital city, including the
preparation of all supporting systems such as infrastructure, human
resources, and governance systems.

\includegraphics[width=6.07292in,height=\textheight]{images/clipboard-3462147358.png}

Figure 1: The Relocation Settings and Vision. source: (Capital Authority
2024)

As the development is still in the initial stage, the detailed planning
documents haven't been launched yet. Thus, I use available published
documents regarding the detail of Nusantara's Development which all of
them are publicly available, such as
\href{https://www.ikn.go.id/storage/pedoman-nusantara/2/nusantara-vlr-baseline-en.pdf}{Nusantara
Sustainable Development Goals (SDGs) Voluntary Local Review Baseline
{[}2024{]}} and
\href{https://ikn.go.id/storage/pedoman-nusantara/1/nusantara-biodiversity-management-master-plan-2024.pdf}{Nusantara
Biodiversity Management Master Plan} {[}2024{]}

\begin{quote}
\textbf{Policy}
\end{quote}

The new capital city, Nusantara, is designed as a \textbf{forest city},
with 75\% of its designated area being green space. This design aims to
create a harmonious blend of urban development and biodiversity hotspots
(Borneo Island, where Nusantara is located, is famous for its tropical
rainforests). However, the design of being a forest city, its proximity
to the rainforest, and its drive on landscape change would present
significant \textbf{challenges}. One of the major concerns is the
increasing likelihood of mosquito-borne diseases (such as
\textbf{malaria}) spreading in the new capital, which are prevalent in
tropical regions Surendra et al. (2024). Since malaria is both a global
and local challenge, certain goals should be considered to support
Nusantara's sustainability, such as:

\textbf{A. Global Goals :} Sustainable Development Goals (SDGs) 3.3 :
Fight Communicable Diseases

The SDGs propose achievable global in combating malaria with target that
in include reducing incident, mortality rates, eliminate malaria in 35
countries by 2030 and prevent resurgence of the disease in a
malaria-free country. Meanwhile, Indonesia's estimated malaria incidence
per 1000 population at risk is still on range between 1-50 incidents per
1000 population in 2023. To achieve target of Global Goals, (WHO 2021)
have launched global technical strategy for malaria with framework such
as:

\includegraphics[width=7.76042in,height=\textheight]{images/clipboard-2410472720.png}

Figure 2 : SDGs Goal and WHO Technical Strategy

\textbf{B.National Level :} National Action Plan for Acceleration of
Malaria Elimination 2020-2026

Translating the global goals on malaria elimination, Indonesia's
Ministry of Health (Ministry of Health and Control 2023) had proposed
recommendations, including the new capital city such as:

\begin{itemize}
\item
  Malaria elimination policies and implementation need basic research,
  operational support, and efficient technology development.
\item
  Malaria elimination policies and implementation need basic research,
  operational support, and efficient technology development
\item
  Planning and implementing malaria elimination activities are based on
  district endemicity stratification.
\item
  Allocating funds from the central government to improve case finding,
  surveillance and vector control around Nusantara area

  \textbf{C.} \textbf{Provincial Level :} Governor Regulation 53/2023
  Guidelines For Malaria Elimination Implementation Number 5 point 2
\item
  Suppress the endemic incidents (high and medium risk)
\item
  Eliminate the number of incidents
\item
  Maintain free malaria status (low-none risk)
\end{itemize}

\hypertarget{application-2}{%
\section{Application}\label{application-2}}

\begin{quote}
Remote Sensing as Baseline for detecting malaria hotspot
\end{quote}

In malaria elimination projects, remote sensing can serve as a crucial
baseline data source for mapping malaria hotspots by integrating
climatic and land-use factors. @wimberly2021 proposed a framework that
leverages Earth observation products to identify mosquito habitats based
on climate conditions, human activities, and specific land-use patterns.

\includegraphics[width=7in,height=\textheight]{images/clipboard-687725868.png}

Figure 2: Framework in which Remote Sensing used in Malaria studies.
source : Wimberly et al. (2021).

To address policy mentioned in section 2, I underlined some dataset that
could be used to the analysis:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1609}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8391}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Data
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \textbf{Sentinel-2 (rainy season)}
\end{itemize}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\item
  \emph{Highlight water bodies and wetlands} -- These serve as proxies
  for mosquito breeding sites.
\item
  \emph{Vegetation and land cover} -- Provides insight into potential
  mosquito habitats.
\item
  \emph{Surface temperature} -- Acts as a proxy for mosquito activity.
\end{itemize}
\end{minipage} \\
\begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \textbf{Digital Elevation Model (DEM) Data/Topography}
\end{itemize}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  Helps to provide topografy to identify potential inundation areas,
  which could influence mosquito breeding patterns.
\end{itemize}
\end{minipage} \\
\begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \textbf{Microsoft Open Buildings}
\end{itemize}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  Useful as a proxy for human settlements and potential exposure risk.
\end{itemize}
\end{minipage} \\
\begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \textbf{Rainfall Data}
\end{itemize}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  The rainfall season can be considered as a timeframe for analysis.
  However, if locally recorded rainfall data from the Indonesian
  Climatic Institution is available, it could help refine the
  identification of rainfall patterns, allowing for a more informed
  selection of the time series.
\end{itemize}
\end{minipage} \\
\end{longtable}

\hypertarget{reflections-1}{%
\section{Reflections}\label{reflections-1}}

During this week, I got a lot of reflections as I finally found lecture
that explicitly bridging the gap of `academics' to real-world policy. My
reflections would be:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Combining remote sensing with GIS

  Since Nusantara is still uninhabited, we could model nearby
  settlements to investigate the remote sensing framework. By combining
  the results with malaria incident data, we can validate our
  classification---analyzing what percentage of high-risk areas have
  recorded incidents and which have not. While global and local malaria
  elimination frameworks mention aggregating incident data and risk
  levels, they do not explicitly emphasize mapping. Using maps, we can
  overlay malaria hotspots with incident data, land use, and
  socio-economic factors. As (Naserrudin, Yong, et al. 2023) notes,
  people are exposed to malaria due to professions that require them to
  venture deeper into the forest.
\item
  Remote Sensing and GIS is good, but enriching the analysis with
  \textbf{affected communities} make it better

  Beyond remote sensing data, incorporating local knowledge can improve
  the analysis. Understanding how affected communities respond to
  malaria provides insight into the effectiveness of mitigation efforts
  (Naserrudin, Lin, et al. 2023). These communities have lived near
  rainforests for generations and are directly affected, making their
  experiences valuable for practical prevention strategies.
\item
  Implementation challenges, the need for collaboration

  One the most important key-takeaway from the lecture is that ``some
  academics papers are too technical, without clearly addressed policy;
  some policy don't include academic findings they could benefit for.''
  This condition lead to a gap between academics and urban governance.
  However, in my observation during my work with the government the
  potential cause is human resources (make the adoption of academics
  finding hard to implement), annual budget cycles (governments
  prioritize immediate results and may be reluctant to invest in the
  long-term experimental processes typical of academia). Bridging the
  gap on malaria prevention requires collaboration and commitment not
  only between epidemiologists, healthcare, and geospatial analysts but
  with the governments to ensure research translates into actionable
  policies.
\end{enumerate}

\hypertarget{references-1}{%
\section{References}\label{references-1}}

\bookmarksetup{startatroot}

\hypertarget{introduction-to-gee}{%
\chapter{Introduction to GEE}\label{introduction-to-gee}}

\hypertarget{summary-4}{%
\section{Summary}\label{summary-4}}

This week's material is all about GEE (Google Earth Engine). In a simple
definition, GEE is a cloud platform that allows us to access satellite
imagery for the whole world and spatial-computation on Google for free.
GEE hosts massive amounts of satellite imagery and we as a user request
their imagery data and analyse it on the cloud-platform without have to
worry about the capability of our local machine.

Basically, GEE has an architecture that collects user input (client
side) and then process this input (server side). In GEE we could manage
both raster and feature (vector) data. We input command on GEE mostly
using Java Script programming language. As someone enthusiastic with
GEE, the need to learn `a new' programming language almost discourages
me, as I worry that I might mix up all the programming languages I have
learnt before.

However, when I looked at the way GEE articulates data structures, I
found it quite similar to Python, with just a few additional keywords,
such as `var' to denote variable. Below are some basic pieces of
information from Andy's lecture about the GEE language that GEE starters
need to know. Check out this introduction from Google if you're still
unconvinced

\includegraphics[width=5.29167in,height=\textheight]{images/clipboard-203956171.png}

Figure 1 : Basic javascript in GEE. source :
\href{https://andrewmaclachlan.github.io/CASA0023/5_GEE_I.html}{Andrew
Maclachan Github}

When I was first introduced to the idea that GEE is efficient in
managing raster data, I was curious about how this efficiency works. For
example, if I want to use Landsat 8 (30 m resolution) to analyze the
whole UK, it would require processing every 30 m pixel across the whole
region, which I assumed would take longer time. However, what makes GEE
faster dealing with those data actually because there is a pyramid of
reduced resolution in GEE, which able to optimizes performance using
lower-resolution versions of the data when full detail is not necessary
and also resampling method to adapt the analysis requirement. Thus when
performing analysis, GEE does not just take the original image
resolution, but GEE adjusts image resolution based on our output needs.
In summary, when you are zooming out and zooming in, GEE retrieves
different resolution of imagery.

As a starter GEE interface is a bit too much, like where should I focus
on because there are 3 main parts of it. Two years ago I even tried to
write my first script on console panel :) because I thought it would be
similar with R Studio. Come to think back on it, GEE has already put the
`section to code' intuitively in a centre called code editor. See the
picture of GEE interface below, if you are like me a starter in GEE the
very first thing that anyone should notice the most is code editor : a
space to write our code and for the other parts let our self become
adjust to it along the time.

\includegraphics[width=7.17708in,height=\textheight]{images/ee-editor_1920.png}

Figure 2 : GEE Interface. source :
\href{https://developers.google.com/earth-engine/tutorials/community/beginners-cookbook}{GEE
Beginer cookbook}

\hypertarget{application-3}{%
\section{Application}\label{application-3}}

\textgreater\textgreater{} \textgreater``\emph{We might not be able to
turn back time, but well\ldots.. we sure can look back using
Landsat''}\textless\textless{}

In this section, I would like to explore the GEE strength on providing
vast archive of imagery to investigate unplanned settlements during
urban transformation projects. The vast historical of imagery enables
researcher to look back at various critical date for development
plans.@yilmaz2024 used collection of Landsat data (Landsat 5, 6,7 ) from
March 1983 to February 2007. The study selected 13 urban renewal project
areas in Istanbul and examined existing project data and archival
satellite images. Supervised classification was performed on 10 urban
renewal project areas using the GEE platform to investigate urbanization
status before development plans.

With analysis of temporal imagery, the result show a contrasting initial
condition of urban renewal's targetted area and its projection. For
instance, in Gaziosmanpasa district it is projected to be a forested
area in the future, however after looking back up to one year before the
approval of the development plans turns out it is highly urbanized area.
The contrasting initial condition and urban renewal project in this area
will present challenge to the success of the plan, as the development of
greening area will fiercely compete with massively built up area.

This finding also underlined the \emph{issues} that projected plan is
not carefully consider the dominant land use of the area, which will
most likely shape the future land use. As a planner, it is not easy to
make sure the execution of plannings we made although we have risk
mitigation in our proposed plan. However, after reading this paper I
realize that we can utilize the historical archive of satellite imagery
in GEE to monitor land use growth pattern before projecting its future
planning target, especially to target building green space or open space
among the growing built up areas.

\includegraphics[width=7.58333in,height=\textheight]{images/clipboard-3598015637.png}

Figure 3 : Comparisan of existing land use and its projection. Source :
(YÄ±lmaz and Alkan 2024)

The case of comparing urban renewal project with old urban areas is
interesting in urban management. However, ({``Urban Renewal Mapping: A
Case Study in Beijing from 2000 to 2020,''} n.d.) underlined the
challenges to get enough information during the process because the lack
of detection methods framework in urban renewal mapping. As we can see
from the previous paper, they use random forest algorithm to identify
urban and non-urban and validate using Kappa coefficients. In this
paper, they propose a complete mapping framework including segmentation
in detecting temporal information of urban renewal. Some key take away
from their proposed framework are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data Preparation : define the urban scope and limit its boundary
  combined with image collections to get time series image stacks
\item
  Identification of old/renewed urban areas : LandTrendr fitting
  (algorithm to detect inter-annual land cover change) + segmentation of
  loss/grain (quantifying the dissimilarity) and random forest
  classification
\item
  Temporal detection : compare extraction strategy with different
  loss/grain and combination of NDVI/NDBI/NDMI + validation samples
\end{enumerate}

Oh wait\ldots.for someone who is not really interested in remote
sensing's application in urban mapping this will be challenging, but
maybe the snapshot of the result below can help you grasp the intuition
of how those steps yield the result. We can see from the left picture
that we could have NDVI index graph to explain the built-up area

\hypertarget{section}{%
\section{\texorpdfstring{\protect\includegraphics[width=7.83333in,height=\textheight]{images/clipboard-2246403998.png}}{}}\label{section}}

Figure 4 : Left : index of NDVI in time series with curve from
LandTrendr ; right : urban renewal and urban old area. Source :
({``Urban Renewal Mapping: A Case Study in Beijing from 2000 to 2020,''}
n.d.)

\hypertarget{reflection-2}{%
\section{Reflection}\label{reflection-2}}

The most exciting part of using Google Earth Engine (GEE) for me is the
ability to share analysis results through web map applications. Clients
are usually more engaged when they can interact with a visual platform
that helps them understand the project better. Remote sensing analysis
is often more complex than regular vector processing, so I imagine the
difficulty on to deliver the results with its key preprocessing step.
What makes it even harder is, we often need to switch between remote
sensing desktop apps and present the results.

By using a web map, we make it easier for clients, even those without a
technical background, to explore and understand the intuition of the
analysis : what layers we have, what imagery we use, what imageries we
combined. The interactive map will provide a clearer view of the
project, allowing clients to see the bigger picture and stay engaged
throughout the process. However, I must admit GEE is only a tool to make
best use of it we need a prior fundamental concept of remote sensing,
such as spectral band, enhancement, fusion, PCA, and other remote
sensing pre-processing steps. Because in every script we write there
should be a thinking process on why we do reduction, why we do
enhancement, basically need our judgement to navigate each steps.

I would also like to highlight the importance of domain knowledge when
using remote sensing sources. As I mentioned before GEE is just a
tool--its ability to provide remarkable insights truly depend on the
person utilizing it. We might use the same satellite imagery, but with
domain knowledge the analysis will yield different results.

For example, \emph{The Economist} uses satellite data to track conflict
in real time (FYI, I come across this in CASA Seminar in 2023 during the
time I prepared my graduate's school application), \emph{Pediatrics} use
remote sensing to understand the association of air pollution with
asthma prevalence. \emph{Archaeologists} can utilize remote sensing
alongside other methods to understand landscape of historical sites.
There are \emph{tons} of use cases, right?!

If you are having curious mind like me, check Dr Ollie Ballinger's
github page \href{https://oballinger.github.io/\#}{here} to see how
powerful remote sensing can be when combined with domain knowledge. Ah
yes, he is also a lecturer in Remote Sensing class.

\hypertarget{references-2}{%
\section{References}\label{references-2}}

\bookmarksetup{startatroot}

\hypertarget{classification-i}{%
\chapter{Classification I}\label{classification-i}}

\hypertarget{summary-5}{%
\section{Summary}\label{summary-5}}

Today's lecture is exploring remote sensing classification. Several
dataset and applications mentioned are:

\begin{itemize}
\item
  Urban expansion/sprawl with landsat data
\item
  Surface temperature (land and sea) with sentinel 3
\item
  Major air pollutants with sentinel 5
\item
  Urban green spaces (high medium res, Lidar)
\item
  Monitoring and illegal practices landsat data
\item
  Forest fires using landsat TM image
\end{itemize}

There are several steps to extract land cover (do classification) from
earth observation data. We usually combined remote sensing with machine
learning techniques that include training data, build classification
model, output. Today's class focus on classification and regression
trees (CART) as a method of classification. It composes with 2 things :

a. Classification trees (yes no/ discrete values)

In this method, we will have predictors and target. The decision is
mapped with conditions that led to yes or no decision

b.Regression trees ( predict continuous dependant variable)

Meanwhile, in linear regression we try to fit into the regression line,
however if we have data that isn't fitted into the data we subset this
data into smaller chunks. When creating a decision tree, we would be mix
all the leafes of several categories, quantified using gini impurity.
And we aim to find the lowest gini impurity. We decide the breaks by
creating imaginary vertical line among the data and identify the lowest
sum of square residuals.

Things to be paid attention at = OVERFITTING, dealing with this could be

a. Set the min number of pixels/ observations, Only split observations
with~\textbf{min number of 20}.

b. Pruning

c. Removes leave, increase alfa, and find the lowest tree score. The
idea is could we keep the accuracy, when we do generalizations

d. Dividing data : training (creating trees) and testing data (applied
the trees with testing data) find the lowest sum residuals.

See the picture :

We could pick some pixels as training data and validation,the machine
learning will predict the value among this train and validation pixels.

Random forest

As decision tree is not good with new data, thus we could use another
method is random forest. I like to use the chatgpt analogy on this:

ð¡ A Decision Tree is like asking one expert for advice.\\
ð² A Random Forest is like asking 100 experts, each with a unique take,
then averaging their answers.

Applying into imagery:

a. Supervised ( giving them a train data)

Support vector machine, the idea is like a logistic regression. ~It
finds the \textbf{best boundary (decision boundary)} that separates
different classes in the data.The benefit of this method is it allows
some misclassification

b. Unsupervised (I have train data, and I want 10 classification method)
\textgreater{} see recording (43')

a. Usually refers to clustering/ k-means.

b. Isodata (same with kmeans but add several inputs such as ( cluster,
iterations)

Repeat until all pixels were classified

\hypertarget{application-4}{%
\section{Application}\label{application-4}}

\hypertarget{reflection-3}{%
\section{Reflection}\label{reflection-3}}

\hypertarget{references-3}{%
\section{References}\label{references-3}}

\bookmarksetup{startatroot}

\hypertarget{classification-ii}{%
\chapter{Classification II}\label{classification-ii}}

\hypertarget{summary-6}{%
\section{Summary}\label{summary-6}}

\hypertarget{summary-7}{%
\section{Summary}\label{summary-7}}

1.~~~~~ This week lecture continues last week course, about
classification.

a.~~~~~ In preprocessing data we use surface reflectance for labelling
but used TOA as SR is up to 2017

b.~~~~~ Dynamic world what did it connect with classifications

c.~~~~~ Accuracy is assessed using confusion matrix

d.~~~~~ Issue? The large mappinf unit 50x50

e.~~~~~ CNN? Not giving much info about which method is good at

2.~~~~~ Object based image analysis

a.~~~~~ Superpixels

b.~~~~~ SegOptim \textgreater{} at the end finding the most pure end
member

Issue : only allowed to select 1 endmember for each landcover, how about
mixture of landcover?

c.~~~~~ MESMA

Many endmember for many landcover class

3.~~~~~ Confusion matrix

-~~~~~~~~~ The confusion matrix consists of predictive accuracy and user
accuracy and its overall accuracy. This is one of accuracy assessment
about the classification of model (the training) and the validated
pixels

-~~~~~~~~~ 100-pa emission

-~~~~~~~~~ 100-ua omission

-~~~~~~~~~ Kappa coefficient \textgreater{} same images different kappas
value, no agreement on how much good kappa value

\hypertarget{application-5}{%
\section{Application}\label{application-5}}

\hypertarget{reflection-4}{%
\section{Reflection}\label{reflection-4}}

\hypertarget{references-4}{%
\section{References}\label{references-4}}

\bookmarksetup{startatroot}

\hypertarget{synthetic-aperture-radar}{%
\chapter{Synthetic Aperture Radar}\label{synthetic-aperture-radar}}

\hypertarget{summary-8}{%
\section{Summary}\label{summary-8}}

Today's lecture explores sensor imagery called Synthetic Aperture Radar
(SAR). Well the name sounds complicated :) The only word that unfamiliar
for me is `aperture', after asking Chatgpt with prompt of `concise and
explain to 15 years old', I learned that aperture is like the opening of
camera, the wider the aperture the less focus the image captured and the
more light it lets in. Meanwhile, radar is an active sensor that emits
energy for illumination. Due to its longer wavelength, it can penetrate
clouds or dust and capture images at night.

If optical images are like our eyes, \textbf{SAR is like a bat},
emitting chirps of sounds to locate a moth. It identifies objects by
listening to the reflected signal. The signal that reflects back to the
satellite is called backscatter. There will be low backscatter if the
signal hits a flat surface and goes off into space. Meanwhile, it will
produce high backscatter when it hits an object and reflects back to the
satellite. SAR images are typically grayscale; the more signal returned
to the satellite, the brighter (higher value) it appears. I remember
that multispectral images have spectral bands, and this also applies to
SAR. There are many bands, but the most commonly used is the C band,
like the bands in Sentinel-1.

\includegraphics[width=7.25in,height=\textheight]{images/clipboard-648766891.png}

Figure 1. SAR Bands with frequency and wavelength. Source :
\href{https://www.earthdata.nasa.gov/learn/earth-observation-data-basics/sar}{NASA}

Radar can collect signals with different polarization. Polarization
describes the direction in which the plane of a transmitted
electromagnetic waves move back and forth. When signal emitted in
vertical (V) and received horizontal it indicated V-H, meanwhile when
the signal both emitted and received horizontal it indicated H-H.
Different surface will respond differently to the polarizations, such
as:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8571}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type of Scattering
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Occurs when\ldots.
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Rough surface scattering & signal interact with irregularities of
surface and most sensitive to VV scattering (example : bare soil or
water) \\
Volume scattering & signal interacts with multiple scatterer such as
leaves in forest, most sensitive to VH or HV \\
Double bounce scattering & signal reflects off two surfaces before
returning back to sensor. This creates strong backscatter and most
sensitive to HH. (example : tree and building). \\
\end{longtable}

Here's the illustration to understand the type of scattering in radar.

\includegraphics[width=4.40625in,height=\textheight]{images/clipboard-4253915995.png}

Figure 3 : Type of scattering. Source :
\href{https://www.earthdata.nasa.gov/learn/earth-observation-data-basics/sar}{NASA}

\hypertarget{application-6}{%
\section{Application}\label{application-6}}

I would like to mention the application of \textbf{optical remote
sensing versus SAR Imagery} in a research area that was initially
unfamiliar to me but has become one of my interest in recent years. This
week's topic would be \textbf{damage detection during conflict.}

During conflict, the need to assess building damage is crucial for
humanitarian relief efforts. However, in the past the detection depended
on eyewitness reports and manual detection. We know that time is
precious during conflict and humanitarian aids. This calls Mueller et
al. (2021) to generate damage monitoring using Very High Resolution
(VHR) satellite imagery, good for its ever-higher resolution and
frequency, and using machine learning techniques. The intuition behind
the method is using Convolutional Neural Networks (CNNs) to learns from
example of destruction (e.g rubble and bomb raters) to make predictions
about other images. Then to address the challenges of limited amount of
labeled data (because sometimes the destruction is sparse or only
limited buildings destroyed), they use label-augmentation technique
which assumed destroyed at a certain time building will remain destroyed
in subsequent time. This assumption helps to create additional labels
for training dataset. The result is shown below, showing damage before
and after a significant heavy weaponry attack in a neighborhood of
Aleppo, with red indicates patch that is highly predicted as destroyed
while green is low prediction score.

\includegraphics[width=5.83333in,height=\textheight]{images/j.jpeg}

Fig 1 : Damage destruction using optical satellite data. Source :
Mueller et al. (2021).

However, I pondered upon the use of VHR imagery in the analysis as it
must be financially expensive, thus not everyone can get the access into
it. It must be nice to have the open access data for this damage
detection, right? and Ollie, our Remote Sensing lecturer, has found the
solution ! He managed to find a new method for building damage detection
using Pixel-Wise T-test (PWTT) and SAR in Sentinel 1. Yes, you read that
right-- it's sentinel 1---which means it is open access ! Using this
algorithm, he could achieve building-level accuracy higher than deep
learning + VHR method (Ballinger 2024). He uses SAR imagery because it
emits a pulse towards the earth and then measure its return signal,
enabling the analysis of how different textural surface respond
differently.

The intuition behind the methodology is that he investigates how much of
the radar signal is reflected back to the satellite, which called
backscatter amplitude. Basically, building and rubble will reflected the
radar signal differently. The algorithm then compares the backscatter
amplitude of each pixel during period before the conflict and after the
conflict. To determine, the change in backscatter is significant enough
to be classified as damage, he uses T-test to know how much the
variation over time and take the largest value detected as a potential
damage in that pixel. See the figure 2 below, the green shows condition
before invasion while the red is after invasion. Meanwhile, the dashed
line showed the average backscatter amplitude with Â±1 standard deviation
before and after the building's destruction.

\includegraphics[width=5.60417in,height=\textheight]{images/clipboard-2990756776.png}

Fig 2 : The intuition behind damage detection using SAR. Source :
Ballinger (2024)

\hypertarget{reflection-5}{%
\section{Reflection}\label{reflection-5}}

\hypertarget{references-5}{%
\section*{References}\label{references-5}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-arshad2023}{}}%
Arshad, Arslan, Junaid Zulfiqar, Muhammad Hassan Zaib, Ahsan Khan, and
Muhammad Jahanzeb Khan. 2023. {``Mapping Socioeconomic Conditions Using
Satellite Imagery: A Computer Vision Approach for Developing
Countries.''} \emph{Journal of Economy and Technology} 1 (November):
144--63. \url{https://doi.org/10.1016/j.ject.2023.11.001}.

\leavevmode\vadjust pre{\hypertarget{ref-ballinger2024}{}}%
Ballinger, Ollie. 2024. {``Open Access Battle Damage Detection via
Pixel-Wise t-Test on Sentinel-1 Imagery,''} May.
\url{https://doi.org/10.48550/ARXIV.2405.06323}.

\leavevmode\vadjust pre{\hypertarget{ref-bappenas2021}{}}%
Bappenas, Kementrian PPN. 2021. {``Buku Saku Pemindahan Ibu Kota
Negara.''}

\leavevmode\vadjust pre{\hypertarget{ref-benabbes2024}{}}%
Ben Abbes, Ali, Jeaneth Machicao, Pedro L. P. CorrÃªa, Alison Specht,
Rodolphe Devillers, Jean P. Ometto, Yasuhisa Kondo, and David Mouillot.
2024. {``DeepWealth: A Generalizable Open-Source Deep Learning Framework
Using Satellite Images for Well-Being Estimation.''} \emph{SoftwareX} 27
(September): 101785. \url{https://doi.org/10.1016/j.softx.2024.101785}.

\leavevmode\vadjust pre{\hypertarget{ref-capitalauthority2024}{}}%
Capital Authority, Nusantara. 2024. {``Nusantara Sustainable Development
GOals (SDGs) Voluntary Local Review Baseline.''}

\leavevmode\vadjust pre{\hypertarget{ref-han2025}{}}%
Han, Yuan, Jianhua He, Xiaoping Du, Xiao Han, and Yaolin Liu. 2025.
{``Reconstructing Urban Vegetation Evolution in China Using Multimodal
Deep Learning and 30-Years Landsat Archive.''} \emph{Urban Forestry \&
Urban Greening} 103 (January): 128582.
\url{https://doi.org/10.1016/j.ufug.2024.128582}.

\leavevmode\vadjust pre{\hypertarget{ref-ministryofhealth2023}{}}%
Ministry of Health, Directorate General of Disease Prevention, and
Control. 2023. {``National Action Plan for Acceleration of Malaria
Elimination 2020-2026 (Revision).''}
\url{https://malaria.kemkes.go.id/sites/default/files/2024-08/National\%20Strategic\%20Plan\%20Revision_Malaria_29\%20Mei\%202023.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-mueller2021}{}}%
Mueller, Hannes, Andre Groeger, Jonathan Hersh, Andrea Matranga, and
Joan Serrat. 2021. {``Monitoring War Destruction from Space Using
Machine Learning.''} \emph{Proceedings of the National Academy of
Sciences of the United States of America} 118 (23): e2025400118.
\url{https://doi.org/10.1073/pnas.2025400118}.

\leavevmode\vadjust pre{\hypertarget{ref-naserrudin2023a}{}}%
Naserrudin, Nurul Athirah, Pauline Yong Pau Lin, April Monroe, Richard
Culleton, Sara Elizabeth Baumann, Shigeharu Sato, Bipin Adhikari, et al.
2023. {``Exploring Barriers to and Facilitators of Malaria Prevention
Practices: A Photovoice Study with Rural Communities at Risk to
Plasmodium Knowlesi Malaria in Sabah, Malaysia.''} \emph{BMC Public
Health} 23 (1): 1316. \url{https://doi.org/10.1186/s12889-023-16173-x}.

\leavevmode\vadjust pre{\hypertarget{ref-naserrudin2023}{}}%
Naserrudin, Nurul Athirah, Pauline Pau Lin Yong, April Monroe, Richard
Culleton, Sara Elizabeth Baumann, Shigeharu Sato, Rozita Hod, Mohammad
Saffree Jeffree, Kamruddin Ahmed, and Mohd Rohaizat Hassan. 2023.
{``Seeing Malaria Through the Eyes of Affected Communities: Using
Photovoice to Document Local Knowledge on Zoonotic Malaria Causation and
Prevention Practices Among Rural Communities Exposed to Plasmodium
Knowlesi Malaria in Northern Borneo Island.''} \emph{Malaria Journal} 22
(1): 166. \url{https://doi.org/10.1186/s12936-023-04603-5}.

\leavevmode\vadjust pre{\hypertarget{ref-surendra2024}{}}%
Surendra, Henry, Bimandra A. Djaafara, Helen D. Prameswari, Dedy
Supriyanto, Ponco Waluyo, Setyo B. Basuki, Herdiana Herdiana, et al.
2024. {``Mitigating Risks of Malaria and Other Vector-Borne Diseases in
the New Capital City of Indonesia.''} \emph{Nature Communications} 15
(1): 10575. \url{https://doi.org/10.1038/s41467-024-54891-x}.

\leavevmode\vadjust pre{\hypertarget{ref-urbanre}{}}%
{``Urban Renewal Mapping: A Case Study in Beijing from 2000 to 2020.''}
n.d. \url{https://spj.science.org/doi/10.34133/remotesensing.0072}.

\leavevmode\vadjust pre{\hypertarget{ref-who2021}{}}%
WHO, World Health Organization. 2021. {``Global Technical Strategy for
Malaria 2016{\textendash}2030, 2021 Update.''}

\leavevmode\vadjust pre{\hypertarget{ref-wimberly2021}{}}%
Wimberly, Michael C., Kirsten M. de Beurs, Tatiana V. Loboda, and
William K. Pan. 2021. {``Satellite Observations and Malaria: New
Opportunities for Research and Applications.''} \emph{Trends in
Parasitology} 37 (6): 525--37.
\url{https://doi.org/10.1016/j.pt.2021.03.003}.

\leavevmode\vadjust pre{\hypertarget{ref-yilmaz2024}{}}%
YÄ±lmaz, Okan, and Mehmet Alkan. 2024. {``Assessing the Impact of
Unplanned Settlements on Urban Renewal Projects with GEE.''}
\emph{Habitat International} 149 (July): 103095.
\url{https://doi.org/10.1016/j.habitatint.2024.103095}.

\end{CSLReferences}



\end{document}
